{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":13547296,"datasetId":8603850,"databundleVersionId":14273302},{"sourceType":"datasetVersion","sourceId":13547257,"datasetId":8603817,"databundleVersionId":14273260},{"sourceType":"datasetVersion","sourceId":13547266,"datasetId":8603825,"databundleVersionId":14273269}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Gradient-based top genes per class (try shap.GradientExplainer, fallback to manual gradient-saliency)\n# Paste & run in Kaggle. Adjust paths if needed.\n\nimport os, json, joblib, time, math\nimport numpy as np, pandas as pd\nimport torch, torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import LabelEncoder\n\n# ---------- CONFIG ----------\nMULTI_GA_CSV = \"/kaggle/input/genetic-algorithms/Multi GA.csv\"\nSCALER_MULTI_PATH = \"/kaggle/input/saint-multi/scaler_saint_multi.pkl\"\nSAINT_PTH = \"/kaggle/input/saint-multi/saint_multiclass_best.pth\"\nLABEL_ENCODER_PATH = \"/kaggle/input/saint-multi/label_encoder_saint_multi.pkl\"\nOUT_JSON = \"/kaggle/working/top_genes_gradient.json\"\n\nTOPK = 20\nBATCH = 128            # batch for gradient computation\nSAMPLE_FRACTION = 1.0  # reduce <1.0 if you want fewer samples (faster)\nUSE_SHAP_GRAD = True   # try shap.GradientExplainer first\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# ---------- LOAD DATA ----------\ndf = pd.read_csv(MULTI_GA_CSV, low_memory=False)\nprint(\"Loaded Multi GA CSV:\", df.shape)\n\n# detect label column\nlabel_candidates = [c for c in df.columns if \"cancer\" in c.lower() or \"type\" in c.lower() or \"label\" in c.lower()]\nif not label_candidates:\n    raise SystemExit(\"No label column detected in Multi GA CSV.\")\nlabel_col = label_candidates[0]\nprint(\"Using label column:\", label_col)\n\n# feature dataframe (drop label)\nXdf = df.drop(columns=[label_col]).copy()\n\n# ---------- ALIGN TO SAVED SCALER ----------\nif not os.path.exists(SCALER_MULTI_PATH):\n    raise SystemExit(f\"Scaler not found at {SCALER_MULTI_PATH}\")\n\nscaler = joblib.load(SCALER_MULTI_PATH)\nif not hasattr(scaler, \"feature_names_in_\"):\n    raise SystemExit(\"Saved multi scaler must contain feature_names_in_ (was fitted on named columns).\")\n\nscaler_cols = [str(x).strip() for x in scaler.feature_names_in_]\n# check and add missing columns as zero if needed (safe fallback)\nmissing = [c for c in scaler_cols if c not in Xdf.columns]\nif missing:\n    print(f\"Warning: {len(missing)} scaler columns missing in CSV. Filling them with zeros (first 10 shown): {missing[:10]}\")\n    for c in missing:\n        Xdf[c] = 0.0\n\n# reorder to match scaler\nXdf = Xdf.loc[:, scaler_cols]\n# optional sampling\nif SAMPLE_FRACTION < 1.0:\n    Xdf = Xdf.sample(frac=SAMPLE_FRACTION, random_state=42).reset_index(drop=True)\nelse:\n    Xdf = Xdf.reset_index(drop=True)\n\n# fill NaNs\nif Xdf.isna().sum().sum() > 0:\n    Xdf = Xdf.fillna(Xdf.median())\n\n# scale\ntry:\n    X = scaler.transform(Xdf.values.astype(float))\nexcept Exception:\n    X = scaler.transform(Xdf)\n\nprint(\"Prepared X shape:\", X.shape)\n\n# ---------- label encoder ----------\nif os.path.exists(LABEL_ENCODER_PATH):\n    label_enc = joblib.load(LABEL_ENCODER_PATH)\nelse:\n    label_enc = LabelEncoder(); label_enc.fit(df[label_col].astype(str).values)\nclasses = list(label_enc.classes_)\nn_classes = len(classes)\nprint(\"Classes:\", n_classes)\n\n# ---------- SAINT model skeleton ----------\nclass SAINT(nn.Module):\n    def __init__(self, input_dim, num_classes, embed_dim=128, num_heads=4, num_layers=4, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Linear(input_dim, embed_dim)\n        self.layers = nn.ModuleList([ nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout) for _ in range(num_layers) ])\n        self.fc = nn.Linear(embed_dim, num_classes)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        x = self.embedding(x).unsqueeze(0)\n        for layer in self.layers:\n            x = layer(x)\n        x = x.mean(dim=0)\n        return self.fc(self.dropout(x))\n\n# load checkpoint\nsaint_blob = torch.load(SAINT_PTH, map_location=\"cpu\")\nstate = saint_blob.get(\"state_dict\", saint_blob) if isinstance(saint_blob, dict) else None\nmodel = SAINT(input_dim=X.shape[1], num_classes=n_classes).to(device)\nif state is not None:\n    sd = {k.replace(\"module.\",\"\"): v for k,v in state.items()}\n    model.load_state_dict(sd, strict=False)\nmodel.eval()\nprint(\"SAINT model ready on\", device)\n\n# ---------- Try shap.GradientExplainer (fast) ----------\nuse_shap = False\nif USE_SHAP_GRAD:\n    try:\n        import shap\n        print(\"shap version:\", shap.__version__)\n        # sample small background for explainer\n        bg_size = min(200, max(20, int(0.02 * X.shape[0])))\n        rng = np.random.RandomState(42)\n        bg_idx = rng.choice(X.shape[0], size=bg_size, replace=False)\n        background = X[bg_idx]\n        # shap GradientExplainer wants model function that returns logits or outputs\n        def model_fn(x_array):\n            with torch.no_grad():\n                t = torch.tensor(x_array, dtype=torch.float32).to(device)\n                out = model(t)\n                if isinstance(out, torch.Tensor):\n                    return out.cpu().numpy()\n                return out\n        explainer = shap.GradientExplainer(model_fn, background)\n        use_shap = True\n        print(\"Created shap.GradientExplainer with background size\", background.shape[0])\n    except Exception as e:\n        print(\"shap.GradientExplainer not available or failed:\", str(e))\n        use_shap = False\n\nresults = {}\nfeat_names = scaler_cols\n\nif use_shap:\n    print(\"Running GradientExplainer for all classes (this is faster than KernelExplainer).\")\n    # We'll compute mean absolute attributions per class\n    # For memory/speed, iterate over classes and compute shap values for a subset of rows (or all rows if possible)\n    # We'll use batches to avoid OOM\n    all_indices = np.arange(X.shape[0])\n    # optionally limit sample count per class to speed up; here we use all rows\n    start_time = time.time()\n    # shap returns per-class attribution for multi-output models; we request shap values for whole dataset in batches\n    batch = 256\n    sum_abs_by_class = np.zeros((n_classes, X.shape[1]), dtype=float)\n    count_by_class = np.zeros(n_classes, dtype=int)\n    for i in range(0, X.shape[0], batch):\n        xb = X[i:i+batch]\n        # shap values shape: list of arrays (one per output) or array (batch, features, outputs)? handle both\n        sv = explainer.shap_values(xb)  # try: returns list len n_outputs each (batch, features)\n        if isinstance(sv, list):\n            # sv[j] = (batch, features) for class j\n            for j in range(n_classes):\n                arr = np.array(sv[j])\n                sum_abs_by_class[j] += np.sum(np.abs(arr), axis=0)\n            # increment counts\n            for idx in range(i, min(i+batch, X.shape[0])):\n                # determine true label if present (we don't require it; just count samples)\n                count_by_class += 0  # we will normalize by number of samples later (we used all rows)\n        else:\n            # sv shape could be (batch, features, n_outputs)\n            arr3 = np.array(sv)  # shape (batch, features, n_outputs)\n            if arr3.ndim == 3:\n                # move outputs to first axis\n                arr3 = np.transpose(arr3, (2,0,1))  # (n_outputs, batch, features)\n                for j in range(n_classes):\n                    sum_abs_by_class[j] += np.sum(np.abs(arr3[j]), axis=0)\n            else:\n                raise RuntimeError(\"Unexpected shap output shape: \" + str(arr3.shape))\n    elapsed = time.time() - start_time\n    print(\"GradientExplainer completed batches in {:.1f}s\".format(elapsed))\n    # normalize by total number of samples (we summed abs across all samples)\n    total_samples = X.shape[0]\n    mean_abs_by_class = sum_abs_by_class / float(total_samples)\n    # prepare topk per class\n    for c in range(n_classes):\n        vec = mean_abs_by_class[c]\n        order = np.argsort(vec)[::-1][:TOPK]\n        total = vec.sum() + 1e-12\n        top_list = [{\"gene\": feat_names[int(i)], \"mean_abs\": float(vec[int(i)]), \"pct_total\": float(100.0 * vec[int(i)] / total)} for i in order]\n        results[c] = {\"class_name\": str(label_enc.inverse_transform([int(c)])[0]), \"top_genes\": top_list}\nelse:\n    # ---------- FALLBACK: direct mean-absolute-gradient saliency (fast, reliable) ----------\n    print(\"Falling back to manual gradient-saliency (mean abs gradient per class). This is fast and recommended.\")\n    model.eval()\n    n = X.shape[0]\n    counts = np.zeros(n_classes, dtype=int)\n    imp_sum = np.zeros((n_classes, X.shape[1]), dtype=float)\n    X_t = torch.tensor(X, dtype=torch.float32).to(device)\n    # process in batches; for each batch compute logits then gradients for each class\n    for i in range(0, n, BATCH):\n        xb = X_t[i:i+BATCH].clone().detach().requires_grad_(True)\n        logits = model(xb)            # (batch, n_classes)\n        bs = xb.shape[0]\n        # for each class, compute gradient of sum(logit[:,class]) wrt xb\n        for cls in range(n_classes):\n            model.zero_grad()\n            # sum of logits for that class over the batch\n            s = logits[:, cls].sum()\n            s.backward(retain_graph=True)\n            grads = xb.grad.detach().cpu().numpy()  # (batch, features)\n            absmean = np.mean(np.abs(grads), axis=0)  # mean abs gradient per feature for this batch\n            imp_sum[cls] += absmean * bs\n            xb.grad.zero_()\n        # accumulate counts by actual class labels if label column present\n        # if you want class-sampled normalization, compute counts separately; here we normalize by dataset size\n    # normalize by total number of samples used\n    total_samples = float(n)\n    mean_abs_by_class = imp_sum / total_samples\n    for c in range(n_classes):\n        vec = mean_abs_by_class[c]\n        order = np.argsort(vec)[::-1][:TOPK]\n        total = vec.sum() + 1e-12\n        top_list = [{\"gene\": feat_names[int(i)], \"score\": float(vec[int(i)]), \"pct_total\": float(100.0 * vec[int(i)] / total)} for i in order]\n        results[c] = {\"class_name\": str(label_enc.inverse_transform([int(c)])[0]), \"top_genes\": top_list}\n\n# ---------- SAVE RESULTS ----------\nwith open(OUT_JSON, \"w\") as f:\n    json.dump({\n        \"method\": \"shap_gradient_explainer\" if use_shap else \"gradient_saliency\",\n        \"n_rows_used\": int(X.shape[0]),\n        \"n_features\": int(X.shape[1]),\n        \"topk\": TOPK,\n        \"results\": results\n    }, f, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T04:27:44.403385Z","iopub.execute_input":"2025-10-30T04:27:44.404077Z","iopub.status.idle":"2025-10-30T04:28:45.999266Z","shell.execute_reply.started":"2025-10-30T04:27:44.404041Z","shell.execute_reply":"2025-10-30T04:28:45.998678Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nLoaded Multi GA CSV: (10459, 8972)\nUsing label column: cancer_type\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.7.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Prepared X shape: (10459, 8971)\nClasses: 33\nSAINT model ready on cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelEncoder from version 1.7.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"shap version: 0.44.1\n","output_type":"stream"},{"name":"stderr","text":"2025-10-30 04:28:18.798372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761798498.962723      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761798499.018409      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"shap.GradientExplainer not available or failed: <class 'function'> is not currently a supported model type!\nFalling back to manual gradient-saliency (mean abs gradient per class). This is fast and recommended.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import json, os\nOUT_JSON = \"/kaggle/working/top_genes_gradient.json\"\n\nif not os.path.exists(OUT_JSON):\n    raise SystemExit(f\"{OUT_JSON} not found. Make sure the gradient cell finished and saved the file.\")\n\ndata = json.load(open(OUT_JSON))\nprint(\"Method:\", data.get(\"method\", \"gradient_saliency\"))\nprint(\"Rows used:\", data.get(\"n_rows_used\"))\nprint(\"Features:\", data.get(\"n_features\"))\nprint(\"Top-k:\", data.get(\"topk\"))\nprint()\n\nresults = data.get(\"results\", data) if \"results\" in data else data\nfor k in list(results.keys()):\n    cls = results[k][\"class_name\"]\n    genes = [g[\"gene\"] for g in results[k][\"top_genes\"][:10]]\n    print(f\"Class {k} - {cls} -> top 10 genes: {genes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T04:34:46.464183Z","iopub.execute_input":"2025-10-30T04:34:46.464776Z","iopub.status.idle":"2025-10-30T04:34:46.474490Z","shell.execute_reply.started":"2025-10-30T04:34:46.464752Z","shell.execute_reply":"2025-10-30T04:34:46.473691Z"}},"outputs":[{"name":"stdout","text":"Method: gradient_saliency\nRows used: 10459\nFeatures: 8971\nTop-k: 20\n\nClass 0 - TCGA-ACC -> top 10 genes: ['PYY', 'NAMA', 'NXPE1', 'REN', 'NOX1_AMDP1_TRPC6P_TCEB1P24_DDX11L16_WASIR1', 'HOXD_AS2', 'NXPE4', 'TMEM255B', 'TMIGD1', 'RETNLB']\nClass 1 - TCGA-BLCA -> top 10 genes: ['IGBP1P1', 'AC006026_10', 'RPL13AP6', 'PSMB11', 'PIN1P1', 'AC105339_1', 'UBE2L3', 'RPL13A', 'PPIAP30', 'IL37']\nClass 2 - TCGA-BRCA -> top 10 genes: ['AFP', 'KDM5D', 'NAPSA', 'UTY', 'ITLN2', 'APOD', 'RP11_417E7_2', 'SFTPC', 'SFTPA2', 'RP11_344E13_3']\nClass 3 - TCGA-CESC -> top 10 genes: ['PENK', 'RPL19P12', 'NBPF8', 'MIR7_3HG', 'SLC18A1', 'TCF21', 'PCSK2', 'NBPF20', 'HBBP1', 'NBPF10']\nClass 4 - TCGA-CHOL -> top 10 genes: ['NXPE1', 'GALNT14', 'PAX1', 'NOX1_AMDP1_TRPC6P_TCEB1P24_DDX11L16_WASIR1', 'SLC26A3', 'RP11_163N6_2', 'RAB34', 'NXPE4', 'ABCC11_RP11_3M1_1', 'NINL']\nClass 5 - TCGA-COAD -> top 10 genes: ['TMA7', 'MIR101_2', 'STAU2_AS1', 'USP32P1', 'SULT2A1', 'MGARP', 'INS', 'LYZL6', 'AC006552_1', 'RPL19P12']\nClass 6 - TCGA-DLBC -> top 10 genes: ['SPCS2', 'RPS27', 'ZRSR2', 'PAX1', 'RP11_2A4_3', 'HMHB1', 'MT1B_RP11_249C24_11', 'RP4_662A9_2', 'PIPSL', 'KIR3DL1']\nClass 7 - TCGA-ESCA -> top 10 genes: ['PRAC1', 'HOXD_AS2', 'RPL13A', 'RP11_597K23_2', 'AL133481_1_RP11_342M3_2', 'RPS18', 'SPCS2', 'H2BFS', 'RPL6', 'AC010127_5']\nClass 8 - TCGA-GBM -> top 10 genes: ['SYCP2', 'ABCC11_RP11_3M1_1', 'HOXA9_RP1_170O19_20', 'PNLIP', 'ACTG2', 'HOXA5', 'PRAC2', 'POM121L10P', 'RPL19P12', 'OAT']\nClass 9 - TCGA-HNSC -> top 10 genes: ['PGAM2', 'PAX1', 'NDUFC1_MGARP', 'PSMB11', 'RP11_144L1_4', 'MGARP', 'LINC00410', 'PRSS16', 'HOXA9_RP1_170O19_20', 'ADAM32']\nClass 10 - TCGA-KICH -> top 10 genes: ['UTY', 'TMSB4Y', 'KDM5D', 'TTTY14', 'TCF21', 'TTTY1', 'HSD17B11_RP11_529H2_2', 'ZRSR2', 'LILRA3', 'RPL26L1']\nClass 11 - TCGA-KIRC -> top 10 genes: ['HAND2', 'MT1B_RP11_249C24_11', 'INS', 'SLC18A1', 'OR7E47P', 'SST', 'PHOX2B', 'RP11_97O12_2', 'PRAC2', 'RGS4']\nClass 12 - TCGA-KIRP -> top 10 genes: ['RPL19P12', 'TMA7', 'RPL6', 'AC006026_10', 'PIN1P1', 'PIPSL', 'PSMG3', 'RPS27', 'RPL23A', 'LHCGR']\nClass 13 - TCGA-LAML -> top 10 genes: ['HAL', 'LGSN', 'MIR122', 'MUC21', 'STC2', 'STEAP1B', 'PLD5', 'PYY', 'SLC22A1', 'HULC']\nClass 14 - TCGA-LGG -> top 10 genes: ['ZSWIM1', 'RPL39L', 'ACTC1', 'OR6S1', 'TMA7', 'KCNK9', 'HAND2', 'RP11_960L18_1', 'GNG8', 'UQCRBP1']\nClass 15 - TCGA-LIHC -> top 10 genes: ['LGSN', 'TMA7', 'LINC00937', 'TNFRSF25_PLEKHG5', 'IL37', 'OR10AD1', 'RP5_836N10_1', 'NKX2_1_AS1', 'OR6W1P', 'KCNJ14']\nClass 16 - TCGA-LUAD -> top 10 genes: ['INS', 'RP5_921G16_1', 'GUCA2A', 'NOX1_AMDP1_TRPC6P_TCEB1P24_DDX11L16_WASIR1', 'NPY', 'LINC00595', 'RP11_497E19_1', 'PENK', 'SLC18A1', 'NXPE1']\nClass 17 - TCGA-LUSC -> top 10 genes: ['NOX1_AMDP1_TRPC6P_TCEB1P24_DDX11L16_WASIR1', 'NXPE4', 'NXPE1', 'RETNLB', 'IL9R_SPRY3_VAMP7_WASH6P_DPH3P2_TDGF1P3_AJ271736_10', 'NLGN4Y', 'IL1F10', 'KDM5D', 'UTY', 'RPS27']\nClass 18 - TCGA-MESO -> top 10 genes: ['MGARP', 'NDUFC1_MGARP', 'MIR122', 'NOX1_AMDP1_TRPC6P_TCEB1P24_DDX11L16_WASIR1', 'RETNLB', 'SCXA', 'NXPE4', 'SCXB', 'RP11_778J15_1', 'SHCBP1L']\nClass 19 - TCGA-OV -> top 10 genes: ['SFTPA2', 'RPL19P12', 'SSBP3', 'SFTPA1', 'UBE2L3', 'TG', 'RSPH6A', 'RPL23A', 'RPS4XP16', 'PIPSL']\nClass 20 - TCGA-PAAD -> top 10 genes: ['RP11_432J24_5', 'PAX1', 'HOXA9_RP1_170O19_20', 'MGARP', 'IL1F10', 'NDUFC1_MGARP', 'SYNGR1', 'IL9R_SPRY3_VAMP7_WASH6P_DPH3P2_TDGF1P3_AJ271736_10', 'SFTPC', 'NAPSA']\nClass 21 - TCGA-PCPG -> top 10 genes: ['TMIGD1', 'FMO2', 'HOXA9_RP1_170O19_20', 'ZIC4', 'HOXD_AS2', 'M1AP', 'UCA1', 'IL19', 'LGSN', 'IGFL4']\nClass 22 - TCGA-PRAD -> top 10 genes: ['RNF212', 'HULC', 'RPL39L', 'SULT2A1', 'MIR122', 'GATA3_AS1_RP11_379F12_3', 'NOG', 'PINLYP', 'APOA2', 'RP11_432J24_5']\nClass 23 - TCGA-READ -> top 10 genes: ['PIPSL', 'AC006026_10', 'RPL19P12', 'TMA7', 'IL1F10', 'SPCS2', 'RPS27', 'TGM3', 'RPL27A', 'HSP90AB3P']\nClass 24 - TCGA-SARC -> top 10 genes: ['IGBP1P1', 'NBPF8', 'SLC18A1', 'NBPF20', 'RPL13A', 'HOXA9_RP1_170O19_20', 'HNRNPA3', 'NPIPA1', 'SNORA2B', 'NBPF10']\nClass 25 - TCGA-SKCM -> top 10 genes: ['PINLYP', 'LGR5', 'RP11_2A4_3', 'RP11_351J23_1', 'NOX1_AMDP1_TRPC6P_TCEB1P24_DDX11L16_WASIR1', 'RNF212', 'HMSD', 'MMP20', 'NKX3_2', 'PRDM12']\nClass 26 - TCGA-STAD -> top 10 genes: ['MIR122', 'HULC', 'AK1', 'A1BG_AS1', 'MASP2', 'APOA2', 'AFP', 'IGBP1P1', 'AC018799_1', 'SFTA3']\nClass 27 - TCGA-TGCT -> top 10 genes: ['RNF212', 'IGBP1P1', 'PIPSL', 'AC006026_10', 'PRG4', 'ZRANB2_AS2', 'RPL6', 'GCSH_RP11_303E16_8', 'TMA7', 'KIF4B']\nClass 28 - TCGA-THCA -> top 10 genes: ['PHACTR3', 'UTY', 'KDM5D', 'SFTPA2', 'SFTPA1', 'FOLR4', 'NOX1_AMDP1_TRPC6P_TCEB1P24_DDX11L16_WASIR1', 'SLC25A52', 'USP29', 'NOS2']\nClass 29 - TCGA-THYM -> top 10 genes: ['HOXA9_RP1_170O19_20', 'LHCGR', 'SFTPA1', 'RPS27A', 'ACTC1', 'GJC3', 'SFTPC', 'RP11_364P22_1', 'RPS18', 'LILRP2']\nClass 30 - TCGA-UCEC -> top 10 genes: ['LILRA3', 'ACTG2', 'RP3_332B22_1', 'HTR3D', 'GATA5', 'SAA3P', 'KLK5', 'TCF21', 'LIPE', 'GDF15']\nClass 31 - TCGA-UCS -> top 10 genes: ['HBBP1', 'NBPF8', 'NBPF20', 'MT1B_RP11_249C24_11', 'RPL13A', 'PPIAP30', 'NBPF10', 'AC008132_13_snoU13', 'AFP', 'MIR122']\nClass 32 - TCGA-UVM -> top 10 genes: ['HULC', 'P2RX6P', 'GALNT14', 'NKX3_2', 'SOX13', 'RP11_712B9_2', 'MIR122', 'RNLS', 'AC092198_1', 'KNG1']\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# by TCGA name\ntcga_name = \"TCGA-BRCA\"   # change\n# by index\ncls_idx = None            # or set to integer, e.g. 2\n\ndata = json.load(open(\"/kaggle/working/top_genes_gradient.json\"))\nresults = data.get(\"results\", data) if \"results\" in data else data\n\nif cls_idx is None:\n    # find by name\n    found = None\n    for k,v in results.items():\n        if v[\"class_name\"] == tcga_name:\n            found = (k,v); break\n    if found is None:\n        raise SystemExit(f\"{tcga_name} not found in results. Available classes: {[v['class_name'] for v in results.values()]}\")\n    k,v = found\nelse:\n    k = str(cls_idx) if isinstance(list(results.keys())[0], str) else cls_idx\n    v = results[k]\n\nprint(\"Class:\", v[\"class_name\"])\nfor i,g in enumerate(v[\"top_genes\"]):\n    print(i+1, g[\"gene\"], f\"score={g.get('score', g.get('mean_abs', None)):.4e}\", f\"pct={g.get('pct_total',0):.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T04:35:16.805926Z","iopub.execute_input":"2025-10-30T04:35:16.806437Z","iopub.status.idle":"2025-10-30T04:35:16.814846Z","shell.execute_reply.started":"2025-10-30T04:35:16.806415Z","shell.execute_reply":"2025-10-30T04:35:16.814101Z"}},"outputs":[{"name":"stdout","text":"Class: TCGA-BRCA\n1 AFP score=5.8962e-03 pct=0.07%\n2 KDM5D score=5.5705e-03 pct=0.07%\n3 NAPSA score=5.0143e-03 pct=0.06%\n4 UTY score=4.9115e-03 pct=0.06%\n5 ITLN2 score=4.8846e-03 pct=0.06%\n6 APOD score=4.8188e-03 pct=0.06%\n7 RP11_417E7_2 score=4.7069e-03 pct=0.06%\n8 SFTPC score=4.5862e-03 pct=0.05%\n9 SFTPA2 score=4.5236e-03 pct=0.05%\n10 RP11_344E13_3 score=4.2508e-03 pct=0.05%\n11 SFTPA1 score=4.2006e-03 pct=0.05%\n12 PPP1R14A score=4.1656e-03 pct=0.05%\n13 ADIPOQ score=4.1556e-03 pct=0.05%\n14 SNORA14A score=4.1205e-03 pct=0.05%\n15 RP11_432J24_5 score=4.1190e-03 pct=0.05%\n16 NKX3_2 score=4.0435e-03 pct=0.05%\n17 HBA2 score=4.0203e-03 pct=0.05%\n18 TMSB4Y score=4.0139e-03 pct=0.05%\n19 NLGN4Y score=3.9703e-03 pct=0.05%\n20 AC016683_6 score=3.9435e-03 pct=0.05%\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import csv, json\ndata = json.load(open(\"/kaggle/working/top_genes_gradient.json\"))\nresults = data.get(\"results\", data) if \"results\" in data else data\nout_csv = \"/kaggle/working/top_genes_all_classes.csv\"\nrows = []\nfor k,v in results.items():\n    class_name = v[\"class_name\"]\n    for rank, g in enumerate(v[\"top_genes\"], start=1):\n        score = g.get(\"score\", g.get(\"mean_abs\", g.get(\"mean_abs_shap\", None)))\n        pct = g.get(\"pct_total\", None)\n        rows.append([k, class_name, g[\"gene\"], rank, score, pct])\nwith open(out_csv, \"w\", newline=\"\") as f:\n    writer = csv.writer(f)\n    writer.writerow([\"class_index\",\"class_name\",\"gene\",\"rank\",\"score\",\"pct_total\"])\n    writer.writerows(rows)\nprint(\"Saved CSV to\", out_csv)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T04:35:52.759259Z","iopub.execute_input":"2025-10-30T04:35:52.759828Z","iopub.status.idle":"2025-10-30T04:35:52.770848Z","shell.execute_reply.started":"2025-10-30T04:35:52.759804Z","shell.execute_reply":"2025-10-30T04:35:52.770157Z"}},"outputs":[{"name":"stdout","text":"Saved CSV to /kaggle/working/top_genes_all_classes.csv\n","output_type":"stream"}],"execution_count":6}]}